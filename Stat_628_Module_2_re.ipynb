{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stat 628 Module 2 Report\n",
    "\n",
    "## 1.Motivation\n",
    "\n",
    "Recently, Yelp has released some of its 121 million reviews to the public as a part of the “Yelp Dataset Challenge.” This project analyzes a dataset containing 1.5 million reviews with features whose dimension is truly high dimensional (i.e. text data). The two goals of this project are: \n",
    "\n",
    "1. Find out what makes a review positive or negative based on the review and a small set of attributes\n",
    "\n",
    "2. Propose a prediction model to predict the ratings of reviews based on the text and attributes. \n",
    "\n",
    "We expect to provide useful suggestions for merchants based on our findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Description & Cleaning\n",
    "\n",
    "### 2.1 Data Description\n",
    "\n",
    "The dataset consists of training set and test set, containing 1546379 observations and 1016664 observations, respectively. Each observation has three types of data associated with the review: \n",
    "\n",
    "1. Stars (dependent variable)\n",
    "\n",
    "2. Review text (primary independent variable) \n",
    "\n",
    "3. A small set of covariates that provide supplementary information such as date, city and categories. \n",
    "\n",
    "### 2.2 Data Cleaning\n",
    "\n",
    "In data cleaning step, a series of operations are applied to the raw texts:\n",
    "\n",
    "1.\tPunctuations and Stopwords: Remove all of them. Dictionary of stopwords is provided by NLTK package in python. \n",
    "\n",
    "2.\tNon-English reviews: As they are only a super small portion of whole dataset, we ignore them.\n",
    "\n",
    "3.\tLemmatization: All remaining words is transformed by lemmatize function in python. For example, \"broken\", \"broke\" and \"breaking\" are all converted to \"break\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Polarity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Raings Prediction\n",
    "\n",
    "### 4.1 Model Description\n",
    "\n",
    "Multinomial Logistics Classification(MLC) and Support Vector Regression(SVR) are popular and commonly used in predictive task as they usually produce decent results. In practice, running time of these methods in large dataset are still acceptable. Therefore, we decided to choose them to be our pre-models. \n",
    "\n",
    "Then, 20 pre-models are obtained by tuning the L2-penalty of MLC and SVR. To clarify, they are applied to a huge sparse matrix recording both unigram and bigram from cleaned data.\n",
    "\n",
    "Finally, our final prediction, which achieves 0.62 RMSE, is calculated by combining the results from 20 pre-models.\n",
    "\n",
    "### 4.2 Feature Consruction\n",
    "\n",
    "In order to include as much as possible useful textual information, every single word and every words' combination(bigram) of each comment are all considered and are represented by a super large sparse matrix created by number counting. In addition, to scale data, TF-IDF was applied to the raw sparse. A visual workflow is shown below.\n",
    "\n",
    "### 4.3 Model Fitting\n",
    "\n",
    "It's common sense that models with the different penalty emphasize on different information because the higher the penalty, the more emphasis on shrinkage of coefficients. To choose pre-models for final prediction, any number ranging from 0 to 5 with increment 0.1 each time was selected as penalty for RMSE test. Then, 20 models with lowest RMSE were chosen and luckily 10 of which were MLC with the rest being SVR. \n",
    "\n",
    "According to emsemble learning, mixture of models usually beats any single one. In light of that, Lasso regression and simple average were used for the predictions of 20 pre-models to achieve better performance. For convenience, we denoted their results as Lasso and Mean resepctively. Eventually, our final predictions were calculated by:\n",
    "\n",
    "\\begin{align*}\n",
    "Final \\ Prediction = (Lasso + Mean)/2\n",
    "\\end{align*}\n",
    "\n",
    "### 4.4 Model Evaluations\n",
    "\n",
    "From the graph below, the final emsemble model shows the best predictive power. All models were trained based on 1.5 millions training set and the results were RMSE computed by Kaggle.\n",
    "\n",
    "<img src=\"rmse_barplot.jpeg\" alt=\"Root Mean Square Error\" style=\"width: 500px; height: 300px\"/>\n",
    "\n",
    "\n",
    "* **Strengths**\n",
    " * Our method is quite simple, it doesn't require too many operations on data cleaning and transformation.\n",
    " * Final model has a decent predictive power, which gets 0.624 RMSE in Kaggle competition.\n",
    "\n",
    "* **Weaknesses**\n",
    " * Neural Network should work better on NLP problems mostly, but we failed in parameters' tuning due to time limit. Accuracy will be improved if given more time.\n",
    " * It's a little bit time-consuming to explain our final model. In addition, it's not best emsemble actually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "\n",
    "[1] Multinomial Logistic Classification: https://en.wikipedia.org/wiki/Softmax_function\n",
    "\n",
    "[2] Support Vector Regression: https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "\n",
    "[3] TF-IDF Transformation: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions\n",
    "\n",
    "|Name            | Contribution                | \n",
    "|----------------|-----------------------------| \n",
    "|Yishan Cai| exploratory analysis, text mining in R, extracted features from sentimental lexicons|\n",
    "|Jiashun Cheng| basic data investigation, created a dictionary with scores based on training data, conducted basic features extraction, tuned models' parameters|\n",
    "|Yilun Chen| data cleaning, created the sparse matrix with unigram&bigram features, built up regression and classification models, proposed final model|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
